<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-10-25T13:12:35+07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Hoc T’s Blog</title><subtitle>My learning notes</subtitle><author><name>Hoc T</name></author><entry><title type="html">Rnn Predict Currency Exchange Rate Part1</title><link href="http://localhost:4000/2021/10/24/RNN-predict-currency-exchange-rate-part1.html" rel="alternate" type="text/html" title="Rnn Predict Currency Exchange Rate Part1" /><published>2021-10-24T00:00:00+07:00</published><updated>2021-10-24T00:00:00+07:00</updated><id>http://localhost:4000/2021/10/24/RNN-predict-currency-exchange-rate-part1</id><content type="html" xml:base="http://localhost:4000/2021/10/24/RNN-predict-currency-exchange-rate-part1.html">&lt;h1 id=&quot;predicts-currency-exchangerate-using-rnn&quot;&gt;Predicts currency exchange rate using RNN&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/rnn-part-1/cat_touching_water.gif&quot; alt=&quot;Touching the water&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Fig. 1 The very first micro-step touching the Deep Learning pool&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;table-of-contents&quot;&gt;Table of contents&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#preparations&quot;&gt;Preparations&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#fetch-and-prepare-the-datasources&quot;&gt;Fetch and prepare the datasources&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#build-the-model&quot;&gt;Build the model&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#data-pre-processing&quot;&gt;Data pre-processing&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#train-the-model&quot;&gt;Train the model&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#make-the-prediction-and-evaluate-the-result&quot;&gt;Make the prediction and evaluate the result&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#playing-with-the-configuration&quot;&gt;Playing with the configuration&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#whats-next&quot;&gt;What’s next??&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;We’re going through steps on building a simple model using regression with RNN that can predict the currency exchange rate. In this article, we will use &lt;strong&gt;USD ($)&lt;/strong&gt; and &lt;strong&gt;THB (฿)&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;In this scope of the article, I follow the course (and source-code) of &lt;a href=&quot;https://www.udemy.com/user/ligency-team/&quot;&gt;Ligency-Team&lt;/a&gt;. Therefore, I don’t go into the detail of what is the RNN model, but about how to apply the concept to resolve a real-life problem. If you’re interested in the course, please checkout &lt;a href=&quot;https://www.udemy.com/course/deeplearning/&quot;&gt;Deep Learning A-Z course&lt;/a&gt; from Ligency-Team.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/HocTran/rnn-currency-exchange-rate&quot;&gt;The full source code can be found here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As a beginner, I also have some troubles while building up and running the solution, so I want to share the setup as well. Of course, a little Python experience will help you go faster, but if you don’t have, it’s fine; you still get it quickly with basic programming skills, I promise 😉.&lt;/p&gt;

&lt;p&gt;This article consists of several steps to go:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Prepare the working environment.&lt;/li&gt;
  &lt;li&gt;Get the real exchange rate data and prepare the data set.&lt;/li&gt;
  &lt;li&gt;Build the model.&lt;/li&gt;
  &lt;li&gt;Using the trained model to predict the exchange rate and evaluate the result.&lt;/li&gt;
  &lt;li&gt;Adjust the model and compare the result.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Remark1: The article focuses on using RNN to solve a problem, not optimizing the results.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Remark2: The article is for learning material; the result is not considered a trusted source by any means 😉.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;preparations&quot;&gt; Preparations&lt;/h2&gt;

&lt;p&gt;The code was compiled under &lt;strong&gt;Python v3.9.7&lt;/strong&gt;, with the following packages&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;numpy (1.19.5)&lt;/li&gt;
  &lt;li&gt;pandas (1.3.4)&lt;/li&gt;
  &lt;li&gt;matplotlib (3.4.3)&lt;/li&gt;
  &lt;li&gt;keras (2.4.3)&lt;/li&gt;
  &lt;li&gt;scikit-learn (1.0)&lt;/li&gt;
  &lt;li&gt;tensorflow (2.4.1)&lt;/li&gt;
  &lt;li&gt;(and auto packages dependencies)&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Skip this step if you’re already familiar with the setup.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I had a few troubles setting up my environment; especially I got lots of conflicts while installing the packages. So I end up using &lt;a href=&quot;https://medium.com/r/?url=https%3A%2F%2Fwww.anaconda.com%2Fproducts%2Findividual&quot;&gt;&lt;strong&gt;Anaconda&lt;/strong&gt;&lt;/a&gt; to manage the environment.&lt;/p&gt;

&lt;p&gt;See the Anaconda website to learn more about the detail. Here I wrap-up what I did:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Download and install Anaconda Navigator.&lt;/li&gt;
  &lt;li&gt;(Optional) Create a new working environment. This is strongly recommended (from my perspective). The default env contains many other packages you don’t need for the project, which may lead to dependency conflicts.&lt;/li&gt;
  &lt;li&gt;Then, activate your environment.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Add a new channel &lt;strong&gt;conda-forge&lt;/strong&gt; into the channel list.&lt;/p&gt;

    &lt;p&gt;The default package hub doesn’t have all packages or versions we need, we’ll find them in the channel &lt;strong&gt;conda-forge&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Search and install all packages (listed above)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Launch the code editor from the home page.&lt;/p&gt;

    &lt;p&gt;I preferably select Spyder, which I love its feature ‘Variable Explorer’. But you can choose any.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Alright, that’s it! You’re now set for the coding.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Alternative 1 - You can use Anaconda via the command-line interface.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Alternative 2 - You can use Google Colab; the env is set and ready from the web browser.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;fetch-and-prepare-the-datasources&quot;&gt;Fetch and prepare the datasources&lt;/h2&gt;

&lt;p&gt;In order to feed the train and evaluation, we need data. We will go to the &lt;a href=&quot;https://medium.com/r/?url=https%3A%2F%2Fwww.xe.com%2Fcurrencycharts%2F&quot;&gt;XE.com&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I also attached the train and test sets (rate_train.csv and rate_test.csv) in the repo, you can skip this step and go to pre-processing.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;From XE.com, in the free charts section, select the source and destination currencies, in our case, they’re &lt;strong&gt;USD&lt;/strong&gt; and &lt;strong&gt;THB&lt;/strong&gt; correspondingly. To make the prediction more precisely, we select the data in the past &lt;strong&gt;10 years&lt;/strong&gt;. Then, inspect the network call, and we’ll get the raw json for the rates. You can also find the attached json file in the source code &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;raw_10y.json&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/rnn-part-1/xe_chart.png&quot; alt=&quot;Xe Currency Charts&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Fig. 2 USD to THB exchange rates&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Once we had the raw data, we split it into two parts, one for training and the other one for evaluating.&lt;/p&gt;

&lt;p&gt;Open the &lt;strong&gt;update_data_set.py&lt;/strong&gt;, which contains a few simple steps to split the data.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;#1. Import the libraries
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;json&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;datetime&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#2. Parse the json
&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;file&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'raw_10y.json'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fileData&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;first&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fileData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'batchList'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;startTime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'startTime'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;interval&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'interval'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rates&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rates'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#3. Mapping data
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nRow&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dates&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nRow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;timestamp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;startTime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interval&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fromtimestamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timestamp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strftime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'%Y.%m.%d'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dataframe&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'date'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'rate'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#4. Split to train and test set
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainSize&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nRow&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;trainSet&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataframe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainSize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;testSet&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataframe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainSize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#5. Write to files
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainSet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rate_train.csv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;testSet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rate_test.csv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Step #1&lt;/strong&gt;. Import libraries&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;json lib is to parse json file we recorded earlier.&lt;/li&gt;
  &lt;li&gt;pandas is to write data into csv format (and read it later).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Step #2&lt;/strong&gt;. Parse the json&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;startTime - the time at the beginning, it means 10y ago by the time data was fetched.&lt;/li&gt;
  &lt;li&gt;interval - the time interval between every data row. In our case, it’s one day.&lt;/li&gt;
  &lt;li&gt;rates - they’re rate records for the past 10 years. Note, we drop the first value here, as it doesn’t present a rate.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Step #3&lt;/strong&gt;. Mapping data. Prepare data into 2 columns of &lt;strong&gt;date&lt;/strong&gt; and &lt;strong&gt;rate&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step #4&lt;/strong&gt;. Split data set into the train and test sets. Here we use 80% of the samples (equivalent to the first 8 years) as the train set. The rest is for the test set (2 years).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step #5&lt;/strong&gt;. Write the data to the files.&lt;/p&gt;

&lt;p&gt;Congratulations 🎉! You now have the training set and evaluating set ready.&lt;/p&gt;

&lt;h2 id=&quot;build-the-model&quot;&gt;Build the model&lt;/h2&gt;

&lt;p&gt;Quick recap. Check out this ultimate article about RNN and LSTM.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/rnn-part-1/rnn.png&quot; alt=&quot;RNN&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Fig 3. RNN&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Overview of constants&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;TIME_STEPS&lt;/strong&gt;. The number of LSTM inputs and outputs.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;LSTM_UNITS&lt;/strong&gt;. The number of neurons in LSTM.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;DROPOUT&lt;/strong&gt;. The rate that LSTM will drop when propagating.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;EPOCHS&lt;/strong&gt;. The number of times model iterate the train.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;BATCH_SIZE&lt;/strong&gt;. The number of data to feed in a batch.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At the later point, you can adjust these values to observe the different results.&lt;/p&gt;

&lt;h3 id=&quot;data-pre-processing&quot;&gt;Data pre-processing&lt;/h3&gt;

&lt;p&gt;Even though, we already have training and test set, we need to do a bit more to make the data is feedable for the model.&lt;/p&gt;

&lt;p&gt;During this step, we only work with the train set.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;c1&quot;&gt;#1. Importing the training set
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rate_train.csv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;training_set&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#2. Feature Scaling
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MinMaxScaler&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MinMaxScaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_range&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;training_set_scaled&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#3. Creating a data structure with TIME_STEPS timesteps and 1 output
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TIME_STEPS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_set_scaled&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TIME_STEPS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_set_scaled&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#4 Reshaping
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Step #1&lt;/strong&gt;. Read data from the set. We again use &lt;strong&gt;pandas&lt;/strong&gt; to read the data from the set. The code will get all row values from the rate column in the csv, turn it into the &lt;strong&gt;numpyarray&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step #2&lt;/strong&gt;. Next, we scale the numbers into the bound of [0,1]. There are a few scalers can use, here we involve the &lt;strong&gt;MinMaxScaler&lt;/strong&gt; from &lt;strong&gt;sklearn&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step #3&lt;/strong&gt;. Natively, we convert the set into 2 parts, the &lt;strong&gt;X_train&lt;/strong&gt; stands for the inputs, the &lt;strong&gt;y_train&lt;/strong&gt; stands for the value which the model drives to.&lt;/p&gt;

&lt;p&gt;In our RNN model, it works with the time series, as amount of previous steps will be used as inputs for one current step. Therefore each item in &lt;strong&gt;X_train&lt;/strong&gt; consists of an array with previous &lt;strong&gt;TIME_STEPS&lt;/strong&gt; values. The &lt;strong&gt;y_train&lt;/strong&gt; is simply inserting the plain values.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/rnn-part-1/x_train.png&quot; alt=&quot;X_train&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Alright, pretty much good, but not done yet!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step #4&lt;/strong&gt;. Reshape.&lt;/p&gt;

&lt;p&gt;We need to define the number of indicators for the input. In our case, we only have one column of the rate, so the indicator equals 1. Therefore, in #4, we reshape the &lt;strong&gt;X_train&lt;/strong&gt; with the number of indicators.&lt;/p&gt;

&lt;p&gt;Done! We have good data we need to feed the system.&lt;/p&gt;

&lt;h3 id=&quot;train-the-model&quot;&gt;Train the model&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;#1. Importing the Keras libraries and packages
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.layers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.layers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LSTM&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.layers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dropout&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#2. Initialising the RNN
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;regressor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#3. Adding the first LSTM layer and some Dropout regularisation
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;regressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LSTM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LSTM_UNITS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_sequences&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;regressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DROPOUT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;regressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LSTM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LSTM_UNITS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_sequences&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;regressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DROPOUT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#3.x Adding a few more granular LSTM layer....
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#4. Adding the output layer
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;regressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#5. Compiling the RNN
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;regressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'adam'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'mean_squared_error'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#6. Fitting the RNN to the Training set
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;regressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;EPOCHS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BATCH_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Step #1&lt;/strong&gt;. Using &lt;strong&gt;keras&lt;/strong&gt; to import classes support to build the network.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step #2&lt;/strong&gt;. Initialize the RNN&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step #3&lt;/strong&gt;. Add a number of LSTM layers. Only the first one will get the input shape of the train data, all the following won’t need.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step #4&lt;/strong&gt;. Add the output layer with 1 output.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step #5, #6&lt;/strong&gt;. Compile and train the set.&lt;/p&gt;

&lt;p&gt;It’s time to get a coffee ☕️, and wait for the machine learning…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/rnn-part-1/mr-bean-waiting.gif&quot; alt=&quot;waiting_model_training&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;make-the-prediction-and-evaluate-the-result&quot;&gt;Make the prediction and evaluate the result&lt;/h2&gt;

&lt;p&gt;Phewww! After a while, we finally got the trained regressor and it’s ready for prediction.&lt;/p&gt;

&lt;p&gt;In this section, we will use the trained regressor to predict the exchange rate in 2 years, and compare it with the real data in the test set. This is probably the most exciting part. OK, let’s go 🏃‍♂️!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note, it’s different from the interpolation that tries to get as close as possible to the real value, our RNN is trying to predict the direction from the values it learned in the past. It completely doesn’t know what the real values are when making the prediction. In other words, the real value and the predicted one may be very different!!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;First, we need to load and pre-process the test data. Then, we do almost the same thing with the training set, but apply for the test set. (Load the test set, apply the scaler, and reshape). But then, we don’t feed the &lt;strong&gt;X_test&lt;/strong&gt; into the RNN, but use the trained regressor to predict the values.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;predicted_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;regressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Great, then we got the predicted values!! But wait, if you remember, we have the &lt;strong&gt;MinMaxScaler&lt;/strong&gt; applied on the set, which means the outcome will be scaled within [0,1] bound. Therefore, we need to inverse it to get the real value. To do that, we reuse the same scaler and do the inverse transformation.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;predicted_rate&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inverse_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predicted_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally! Now, it’s time to get a comparison. We use matplotlib to visualize the result in the same space, where we can see the differences between real and prediction values.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/rnn-part-1/prediction-60-50-0.2-5.png&quot; alt=&quot;Prediction&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Fig5. Real and Prediction in a comparison&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Congratulation 🎉, the machine is working! Even though there are a few peaks at some points, generally, the model truly can predict!&lt;/p&gt;

&lt;h2 id=&quot;playing-with-the-configuration&quot;&gt;Playing with the configuration&lt;/h2&gt;

&lt;p&gt;In the source code, we have a few constants defined and adjust them to evaluate and get better (or worse) results. Basically, there are a few points we can think about&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Increase the dataset. Yes, more data will help to train the better model.&lt;/li&gt;
  &lt;li&gt;Increase the number of time steps, where the time item will depend furthermore on the previous one, f.ex 100, 200.&lt;/li&gt;
  &lt;li&gt;Increase the number of layers and number of its neurons.&lt;/li&gt;
  &lt;li&gt;Decrease the rate in the dropout.&lt;/li&gt;
  &lt;li&gt;Increase the number of epochs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let’s see a few adjustments&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/images/rnn-part-1/prediction-30-20-0.1-20.png&quot; alt=&quot;Prediction&quot; /&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/images/rnn-part-1/prediction-60-50-0.2-20.png&quot; alt=&quot;Prediction&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;(30, 20, 0.1, 20)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;(60, 50, 0.2, 20)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/images/rnn-part-1/prediction-60-50-0.1-25.png&quot; alt=&quot;Prediction&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/assets/images/rnn-part-1/prediction-120-100-0.1-100.png&quot; alt=&quot;Prediction&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;(60, 50, 0.1, 25)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;(120, 100, 0.1, 100)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;Fig 6. Configuration = (TIME_STEPS, LSTM_UNITS, DROPOUT, EPOCH)&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next??&lt;/h2&gt;

&lt;p&gt;In this article, we built a model that uses the time interval of 1 day. Therefore, it can predict the value for one day using the number of days in the past. However, what if we want to predict the exchange rate next week or next month 🤔. Then, we need a bit more. Let’s discover it in part 2 😉.&lt;/p&gt;

&lt;p&gt;Diving further! And practicing!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/rnn-part-1/diving.gif&quot; alt=&quot;Diving&quot; /&gt;&lt;/p&gt;</content><author><name>Hoc T</name></author><summary type="html">Predicts currency exchange rate using RNN</summary></entry></feed>